{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction & Goal üéØ\n",
    "\n",
    "## Fine-tuning a Language Model for Translation\n",
    "\n",
    "Welcome! This notebook is your hands-on guide to fine-tuning a pre-trained model for a new task.\n",
    "We'll take a model that understands English and teach it how to translate from **English to French**.\n",
    "\n",
    "### What You'll Learn:\n",
    "1.  **Setup**: How to install and import the necessary libraries.\n",
    "2.  **Load Data**: How to load a standard translation dataset from the Hugging Face Hub.\n",
    "3.  **Preprocess**: How to prepare the text data for the model using a tokenizer.\n",
    "4.  **Fine-Tune**: The core process of training the model on the new data.\n",
    "5.  **Inference**: How to use your newly fine-tuned model to translate sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Setup üõ†Ô∏è | Installing Libraries\n",
    "# First things first, we need to install the libraries that will do the heavy lifting.\n",
    "\n",
    "# - **transformers**: Provides the pre-trained models (like T5) and the training tools.\n",
    "# - **datasets**: Makes it super easy to load datasets from the Hugging Face Hub.\n",
    "# - **sacrebleu**: A standard library for evaluating translation quality.\n",
    "# - **accelerate**: Helps PyTorch (the backend for transformers) run smoothly on GPUs or TPUs.\n",
    "\n",
    "!pip install transformers[torch] datasets sacrebleu accelerate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Loading the Dataset üìö\n",
    "# We need data to teach our model. We'll use the 'opus_books' dataset, which contains pairs of sentences in English and French from translated books.\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load a small part of the dataset to keep training fast for this tutorial.\n",
    "# We'll use the first 1% of the pairs for this example.\n",
    "raw_datasets = load_dataset(\"opus_books\", \"en-fr\", split=\"train[:1%]\")\n",
    "\n",
    "# The dataset is currently one big block. Let's split it into a training set and a testing set.\n",
    "# 90% for training, 10% for testing.\n",
    "split_datasets = raw_datasets.train_test_split(train_size=0.9, seed=42)\n",
    "\n",
    "# Rename the 'test' split to 'validation' which is a more common term in training.\n",
    "split_datasets[\"validation\"] = split_datasets.pop(\"test\")\n",
    "\n",
    "# Let's see what a sample looks like!\n",
    "print(\"A sample from our dataset:\")\n",
    "print(split_datasets[\"train\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Preprocessing the Data ‚úçÔ∏è\n",
    "# Models don't understand words; they understand numbers. The process of converting words to numbers is called \"tokenization\".\n",
    "# We'll use a \"tokenizer\" that was created alongside our pre-trained model to ensure the numbers match what the model expects.\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# The model we'll be fine-tuning is 't5-small'. It's a good balance of size and performance for a tutorial.\n",
    "model_checkpoint = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# T5 is a sequence-to-sequence model. It needs a specific prefix to know what task it should be doing.\n",
    "# For translation, we'll tell it \"translate English to French: \".\n",
    "prefix = \"translate English to French: \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"This function takes a batch of examples and tokenizes them.\"\"\"\n",
    "    # Add the prefix to all the English sentences\n",
    "    inputs = [prefix + doc[\"en\"] for doc in examples[\"translation\"]]\n",
    "\n",
    "    # Get the French sentences\n",
    "    targets = [doc[\"fr\"] for doc in examples[\"translation\"]]\n",
    "\n",
    "    # Tokenize the inputs and targets\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
    "    labels = tokenizer(text_target=targets, max_length=128, truncation=True)\n",
    "\n",
    "    # The 'labels' are what the model should learn to predict.\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Now, apply this function to our entire dataset. The 'map' function is a powerful way to do this quickly.\n",
    "tokenized_datasets = split_datasets.map(preprocess_function, batched=True)\n",
    "\n",
    "print(\"\\nSample of tokenized data (the model sees numbers, not words):\")\n",
    "print(tokenized_datasets[\"train\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Fine-Tuning the Model üî•\n",
    "# This is where the magic happens! We'll load the pre-trained T5 model and set up a \"Trainer\" that handles the entire training loop for us.\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "\n",
    "# Load the pre-trained T5 model.\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "# A data collator is a helper that batches our tokenized data together nicely for the model.\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# Define the training arguments. These are like settings for our training session.\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"t5-small-en-to-fr\",          # Where to save the model\n",
    "    evaluation_strategy=\"epoch\",            # Evaluate at the end of each epoch\n",
    "    learning_rate=2e-5,                     # A standard learning rate for fine-tuning\n",
    "    per_device_train_batch_size=16,         # How many examples to process at once during training\n",
    "    per_device_eval_batch_size=16,          # How many examples to process at once during evaluation\n",
    "    weight_decay=0.01,                      # Helps prevent overfitting\n",
    "    save_total_limit=3,                     # Only keep the best 3 model checkpoints\n",
    "    num_train_epochs=3,                     # We'll train for 3 full passes over the data\n",
    "    predict_with_generate=True,             # Necessary for sequence-to-sequence tasks\n",
    "    push_to_hub=False,                      # Set to True if you want to upload to Hugging Face Hub\n",
    ")\n",
    "\n",
    "# Create the Trainer object.\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Start training! This will take a few minutes on a Colab GPU.\n",
    "# Make sure your runtime is set to GPU (Runtime -> Change runtime type -> T4 GPU)\n",
    "print(\"Starting the fine-tuning process...\")\n",
    "trainer.train()\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Inference (Using Your Model) üó£Ô∏è\n",
    "# The model is trained! Now for the fun part: let's give it an English sentence and see how it does.\n",
    "\n",
    "from transformers import pipeline\n",
    "import os\n",
    "\n",
    "# Find the latest checkpoint directory\n",
    "output_dir = \"t5-small-en-to-fr\"\n",
    "checkpoints = [os.path.join(output_dir, d) for d in os.listdir(output_dir) if d.startswith('checkpoint-')]\n",
    "latest_checkpoint = max(checkpoints, key=os.path.getmtime)\n",
    "\n",
    "print(f\"Loading model from: {latest_checkpoint}\")\n",
    "\n",
    "# Load our model using the pipeline, which simplifies inference.\n",
    "translator = pipeline(\"translation_en_to_fr\", model=latest_checkpoint)\n",
    "\n",
    "# Let's try translating a sentence.\n",
    "english_sentence = \"My name is Arthur, King of the Britons.\"\n",
    "french_translation = translator(english_sentence)\n",
    "\n",
    "print(f\"English: {english_sentence}\")\n",
    "print(f\"Model's French Translation: {french_translation[0]['translation_text']}\")\n",
    "\n",
    "print(\"\\n--- Another example ---\")\n",
    "english_sentence_2 = \"The weather is lovely today.\"\n",
    "french_translation_2 = translator(english_sentence_2)\n",
    "print(f\"English: {english_sentence_2}\")\n",
    "print(f\"Model's French Translation: {french_translation_2[0]['translation_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Conclusion & Next Steps üéâ\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You have successfully fine-tuned a pre-trained T5 model to translate from English to French.\n",
    "\n",
    "### What We Accomplished:\n",
    "- Loaded and prepared a real-world translation dataset.\n",
    "- Tokenized the text so the model could understand it.\n",
    "- Ran a complete fine-tuning process using the Hugging Face Trainer.\n",
    "- Used the final model to generate new translations.\n",
    "\n",
    "### Where to Go From Here:\n",
    "- **Train for longer**: Training for more epochs on more data will improve performance.\n",
    "- **Try a larger model**: Using `t5-base` or `t5-large` will yield much better results (but take longer to train).\n",
    "- **Translate other languages**: Find another dataset on the Hugging Face Hub and try fine-tuning for a different language pair!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}